{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous: [Mathematical Development of the Tree](02.ipynb) | [Table of Contents](00.ipynb) | Next: [The Discrete Process Y](04.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Willow Tree Lattice Handbook\n",
    "\n",
    "## The Linear Programming Problem\n",
    "\n",
    "Apart from vector $\\mathbf{q}$, which is a special case, the sequence of transition matrices $\\big\\{\\mathbf{P}^{(k)}\\big\\}_{k=1}^{n-1}$ is retrieved through _linear programming_ (LP), the hallmark of Curran's methodology.\n",
    "\n",
    "Linear programming is the process of finding a vector $\\mathbf{x}$ that minimises a linear function $\\mathbf{f}^\\text{T}\\mathbf{x}$ subject to one or more of the following conditions: $\\mathbf{Ax}\\leq\\mathbf{b}$, $\\mathbf{Ax}=\\mathbf{b}$, $\\mathbf{l}\\leq \\mathbf{x}\\leq\\mathbf{u}$, where $\\mathbf{A}$ is a matrix of linear constraints, $\\mathbf{b}$ a vector of linear constraints, $\\mathbf{l}$ and $\\mathbf{u}$, respectively, vectors of lower and upper bounds.\n",
    "\n",
    "The willow tree algorithm employs linear programming in order to retrieve a sequence of transition matrices allowing for fast and accurate convergence of a discrete Markov process $Y$ to the standard Brownian motion. $Y=\\{Y_{t_k}:t_k\\geq 0\\}$ is a process which takes on value $z_i\\sqrt{t_k}$ in grid $\\mathbf{G}$ (see [Mathematical Development of the Tree](02.ipynb)) whenever $X_k$, the Markov chain, is in state $i$ at time $t_k$.\n",
    "\n",
    "#### Key Convergence Properties\n",
    "\n",
    "To guarantee convergence, the matrices must possess some properties key to the standard Brownian motion. In particular, the process they define must be a martingale, with conditional variance between two adjacent time periods equal to the time increment, and with a stationary distribution. The last condition states that, regardless of the sequence of realisations $\\{X_k\\}_{k=0}^{n}$, the Markov chain must converge to a unique, asymptotic distribution. In formulae, for each $k$:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\big[Y_{t_{k+1}}\\mid Y_{t_k}\\big]=Y_{t_k}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Var}\\big[Y_{t_{k+1}}\\mid Y_{t_k}\\big]=h_{k,k+1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\big(Y_{t_{k+1}}=z_j\\sqrt{t_{k+1}}\\big)=\\sum_{i_1i_2\\dots i_k} q_{i_1}p_{i_1i_2}^1p_{i_2i_3}^2\\dots p_{i_kj}^k=\\sum_{i_k}q_{i_k}p_{i_kj}^k=q_j\n",
    "$$\n",
    "\n",
    "- The first equation states that the best prediction for process $Y$ at time $t_{k+1}$ is $Y_{t_k}$, the value taken on in the previous time period, and that knowledge of past history of the process is irrelevant.\n",
    "\n",
    "\n",
    "- The second one constrains the conditional variance of $Y$ in the interval $(t_k,t_{k+1})$ to be equal to the time increment, $h_{k,k+1}$.\n",
    "\n",
    "\n",
    "- The third one ensures that the probability to reach a particular state $j$ at time $t_{k+1}$ is equal to $q_j$, the mass assigned to such state at the beginning of the process. This final condition establishes that, for each step $k$, the Markov chain $X_k$ reproduces a particular unconditional probability distribution $\\mathbf{q}$, also known as _stationary distribution_.\n",
    "\n",
    "#### The Linear Programming Problem\n",
    "\n",
    "Vector $\\mathbf{q}$, which stores the transition probabilities from node 0 to each of the $m$ nodes at time $t_1$, possesses, by construction, the required properties. For each of the subsequent transition matrices, the constraints are satisfied if the probabilities are retrieved according to the linear programming problem:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\min_{p_{ij}^k}\\sum_{i=1}^m\\sum_{j=1}^m {p_{ij}^k\\big\\vert z_j\\sqrt{t_{k+1}}-z_i\\sqrt{t_k}\\big\\vert}^3\\\\\n",
    "\\text{subject to:}&\\nonumber\\\\\n",
    "&\\sum_{j=1}^m p_{ij}^k=1 \\qquad \\forall i\\\\\n",
    "&\\sum_{j=1}^m p_{ij}^kz_j\\sqrt{t_{k+1}} = z_i\\sqrt{t_k} \\qquad \\forall i\\\\\n",
    "&\\sum_{j=1}^m p_{ij}^kz_j^2t_{k+1}-z_i^2t_k=h_{k,k+1} \\qquad \\forall i\\\\\n",
    "&\\sum_{i=1}^m q_ip_{ij}^k=q_j \\qquad \\forall j\\\\\n",
    "& p_{ij}^k\\geq 0 \\qquad \\forall i,j\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "with $t_{k+1}=t_k+h_{k,k+1}$. The first equation is called the _objective function_ of the minimisation problem. The function, in this particular form, minimises the expectation of the absolute value of the cubed distance among all possible transitions during a specific time interval. Curran claims that the function allows for a fast and accurate convergence of process $Y$ to the standard Brownian motion, and that powers higher than three, in the formula, lead to identical transition matrices.\n",
    "\n",
    "#### Ho's Reformulation\n",
    "Ho [8] reformulates the problem using matrix notation, which is computer-language oriented. He defines the following vectors:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{1} & =\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & \\dots & 1\n",
    "\\end{bmatrix}\n",
    "^\\text{T}\\\\\n",
    "\\mathbf{z} & =\n",
    "\\begin{bmatrix}\n",
    "z_1 & z_2 & \\dots & z_m\n",
    "\\end{bmatrix}\n",
    "^\\text{T}\\\\\n",
    "\\mathbf{q} & =\n",
    "\\begin{bmatrix}\n",
    "q_1 & q_2 & \\dots & q_m\n",
    "\\end{bmatrix}\n",
    "^\\text{T}\\\\\n",
    "\\mathbf{r} & =\n",
    "\\begin{bmatrix}\n",
    "z_1^2 & z_2^2 & \\dots & z_m^2\n",
    "\\end{bmatrix}\n",
    "^\\text{T}\\\\\n",
    "\\mathbf{\\alpha} & =\n",
    "\\begin{bmatrix}\n",
    "\\frac{h_{t_1,t_2}}{t_1} & \\frac{h_{t_2,t_3}}{t_2} & \\dots & \\frac{h_{t_{n-1},t_n}}{t_{n-1}}\n",
    "\\end{bmatrix}\\\\\n",
    "\\mathbf{\\beta} & =\n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{\\sqrt{1+\\mathbf{\\alpha}(1)}} & \\frac{1}{\\sqrt{1+\\mathbf{\\alpha}(2)}} & \\dots & \\frac{1}{\\sqrt{1+\\mathbf{\\alpha}(n-1)}}\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- $\\mathbb{1}$ is an $m$-valued vector of ones,\n",
    "\n",
    "\n",
    "- $\\mathbf{z}$ and $\\mathbf{q}$ are the (now column) vectors previously defined,\n",
    "\n",
    "\n",
    "- $\\mathbf{r}$ is an $m$-valued array obtained by squaring each component of $\\mathbf{z}$, and\n",
    "\n",
    "\n",
    "- $\\mathbf{\\alpha}$ and $\\mathbf{\\beta}$ are $(n-1)$-valued row vectors which simplify calculations. Their dimension, $(n-1)$, is due to the fact that the length of $\\mathbf{h}$, which appears both on the numerator of $\\mathbf{\\alpha}$ and on the denominator of $\\mathbf{\\beta}$, is $n$, and that $t_0$, the first component of $\\mathbf{t}$, a ($n+1$)-valued array, is omitted in order to avoid division by 0. The notation $\\mathbf{\\alpha}(\\cdot)$ in $\\mathbf{\\beta}$ denotes the position of the chosen element in vector $\\mathbf{\\alpha}$.\n",
    "\n",
    "\n",
    "Ho also rewrites the objective function, disentangling transition matrix $\\mathbf{P}^{(k)}$ from grid $\\mathbf{F}^{(k)}$, defined, for each $k$, as the $m\\times m$ matrix with entries $\\mathbf{F}^{(k)}_{ij}=\\big\\vert z_j - \\mathbf{\\beta}(k)z_i\\big\\vert^3$:\n",
    "\n",
    "$$\n",
    "\\mathbf{F}^{(k)}=\n",
    "\\begin{bmatrix}\n",
    "\\big\\vert z_1 - \\mathbf{\\beta}(k)z_1\\big\\vert^3 & \\big\\vert z_2 - \\mathbf{\\beta}(k)z_1\\big\\vert^3 & \\dots & \\big\\vert z_m - \\mathbf{\\beta}(k)z_1\\big\\vert^3\\\\\n",
    "\\big\\vert z_1 - \\mathbf{\\beta}(k)z_2\\big\\vert^3 & \\big\\vert z_2 - \\mathbf{\\beta}(k)z_2\\big\\vert^3 & \\dots & \\big\\vert z_m - \\mathbf{\\beta}(k)z_2\\big\\vert^3\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\big\\vert z_1 - \\mathbf{\\beta}(k)z_m\\big\\vert^3 & \\big\\vert z_2 - \\mathbf{\\beta}(k)z_m\\big\\vert^3 & \\dots & \\big\\vert z_m - \\mathbf{\\beta}(k)z_m\\big\\vert^3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{\\beta}(k)$ denotes the $k$-th component of vector $\\mathbf{\\beta}$.\n",
    "\n",
    "\n",
    "For each step $(t_k,t_{k+1})$, the LP problem becomes (omitting the superscript in $\\mathbf{P}^{(k)}$ and $\\mathbf{F}^{(k)}$):\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\min_\\mathbf{P} \\text{tr}\\big(\\mathbf{PF}\\big)\\\\\n",
    "\\text{subject to:}&\\nonumber\\\\\n",
    "&\\mathbf{P}\\mathbb{1}=\\mathbb{1}\\\\\n",
    "&\\mathbf{Pz}=\\mathbf{\\beta}(k)\\mathbf{z}\\\\\n",
    "&\\mathbf{Pr}=\\mathbf{\\beta}(k)^2\\mathbf{r}+(1-\\mathbf{\\beta}(k)^2)\\mathbb{1}\\\\\n",
    "&\\mathbf{q}^\\text{T}\\mathbf{P}=\\mathbf{q}^\\text{T}\\\\\n",
    "&p_{ij}^k\\geq 0 \\qquad \\forall i,j\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\"tr\" in the objective function is the _trace_ operator. When applied to a product of square matrices, trace computes the sum over all components of a Hadamard (or entrywise) product of $\\mathbf{P}$ and $\\mathbf{F}$, so that $\\text{tr}\\big(\\mathbf{PF}\\big) = \\sum_{ij}\\big(\\mathbf{P}\\circ\\mathbf{F}\\big)_{ij}=\\sum_{ij}\\mathbf{P}_{ij}\\mathbf{F}_{ij}$, where \n",
    "\n",
    "$$\n",
    "\\mathbf{P}\\circ\\mathbf{F}=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{P}_{11}\\mathbf{F}_{11} & \\mathbf{P}_{12}\\mathbf{F}_{12} & \\dots & \\mathbf{P}_{1m}\\mathbf{F}_{1m}\\\\\n",
    "\\mathbf{P}_{21}\\mathbf{F}_{21} & \\mathbf{P}_{22}\\mathbf{F}_{22} & \\dots & \\mathbf{P}_{2m}\\mathbf{F}_{2m}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\mathbf{P}_{m1}\\mathbf{F}_{m1} & \\mathbf{P}_{m2}\\mathbf{F}_{m2} & \\dots & \\mathbf{P}_{mm}\\mathbf{F}_{mm}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "with $\\mathbf{P}_{ij}\\mathbf{F}_{ij}$ denoting the product between the elements at position $(i,j)$ of the two matrices.\n",
    "\n",
    "#### SciPy Implementation\n",
    "\n",
    "In Python, the linear programming problem needs to be slightly modified in order to account for some restrictions inherent in the built-in functions. Two major issues arise:\n",
    "\n",
    "- First, trace is not a valid objective function, because the program requires that `c`, the array of coefficients ($\\mathbf{c}$ is equivalent to $\\mathbf{f}$ in SciPy), be disjoint from `x`, the solution vector—a condition not met by trace.\n",
    "\n",
    "- Second, `c` and `x` cannot be matrices, and call for vectorisation, a procedure which also impacts on the definition of `A_eq`, the matrix of constraints, and on `b_eq`, the RHS of the equality constraint. \n",
    "\n",
    "In-depth treatment of the topic in [The Willow Tree in Python, part 2](10.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous: [Mathematical Development of the Tree](02.ipynb) | [Table of Contents](00.ipynb) | Next: [The Discrete Process Y](04.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
